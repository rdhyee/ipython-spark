{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import (SparkContext, SparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://spark.apache.org/docs/1.2.0/configuration.html\n",
    "conf = SparkConf()\n",
    "\n",
    "# https://spark.apache.org/faq.html\n",
    "# local[N] or local[*]\n",
    "conf.setMaster(\"local[10]\").setAppName(\"Simple App\")\n",
    "#conf.set(\"spark.cores.max\", \"10\")\n",
    "\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc = SparkContext(master=\"local\", appName=\"Simple App\")\n",
    "r = sc.parallelize(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35655.45431795675"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import factorial, log10\n",
    "\n",
    "fact_sum = r.map(factorial).sum()\n",
    "log10(fact_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stand-alone mode\n",
    "\n",
    "http://spark.apache.org/docs/1.2.0/spark-standalone.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slaves.sh\t\t start-mesos-dispatcher.sh  stop-master.sh\r\n",
      "spark-config.sh\t\t start-shuffle-service.sh   stop-mesos-dispatcher.sh\r\n",
      "spark-daemon.sh\t\t start-slave.sh\t\t    stop-shuffle-service.sh\r\n",
      "spark-daemons.sh\t start-slaves.sh\t    stop-slave.sh\r\n",
      "start-all.sh\t\t start-thriftserver.sh\t    stop-slaves.sh\r\n",
      "start-history-server.sh  stop-all.sh\t\t    stop-thriftserver.sh\r\n",
      "start-master.sh\t\t stop-history-server.sh\r\n"
     ]
    }
   ],
   "source": [
    "!ls /spark/sbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access ./sbin/start-master.sh: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./sbin/start-master.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will move to mesos, because mesos support docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mesos\n",
    "\n",
    "http://spark.apache.org/docs/1.2.0/running-on-mesos.html\n",
    "\n",
    "Maybe try https://elastic.mesosphere.io/  --> but is there a script for this?\n",
    "\n",
    "http://mesos.apache.org/documentation/latest/ec2-scripts/\n",
    "\n",
    "\n",
    "\n",
    "https://digitalocean.mesosphere.com/clusters/new\n",
    "\n",
    "17 steps in https://mesosphere.com/docs/tutorials/run-spark-on-mesos/ ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
