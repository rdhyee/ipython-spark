{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import (SparkContext, SparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "/spark/bin/run-example SparkPi 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "PYSPARK_PYTHON=python /spark/bin/spark-submit /spark/examples/src/main/python/pi.py 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf().\n",
    "              setMaster(\"mesos://zk://10.132.126.37:2181/mesos\").\n",
    "              setAppName(\"RY from jupyter\").\n",
    "              set(\"spark.executor.uri\", \"http://apache.petsads.us/spark/spark-1.2.0/spark-1.2.0-bin-hadoop2.4.tgz\").\n",
    "              set(\"spark.mesos.coarse\", \"true\").\n",
    "              set(\"spark.mesos.executor.home\", \"/spark-1.2.0-bin-hadoop2.4\").\n",
    "              set(\"spark.executor.extraLibraryPath\", \"/usr/lib/hadoop/lib/native\").\n",
    "              set(\"spark.executor.extraJavaOptions\",\"-XX:-UseConcMarkSweepGC\").\n",
    "              set(\"spark.mesos.native.library\", \"/usr/local/lib/libmesos.so\").\n",
    "              set(\"spark.local.ip\", \"10.9.8.6\").\n",
    "              set(\"spark.driver.host\",\"10.9.8.6\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for spark local\n",
    "#  https://spark.apache.org/docs/latest/quick-start.html\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the README.md\n",
    "\n",
    "textFile = sc.textFile(\"/spark/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Actions](https://spark.apache.org/docs/latest/programming-guide.html#actions):  return values\n",
    "* [Transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations):  return another RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# action\n",
    "\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# confirm that the answer using standard lib\n",
    "len(open(\"/spark/README.md\").read().strip().split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first line:  action\n",
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transformation\n",
    "\n",
    "textFile.filter(lambda x: \"Spark\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many lines with \"Spark\"\n",
    "\n",
    "textFile.filter(lambda x: \"Spark\" in x).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of words in the line with most words\n",
    "textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile.map(lambda line: len(line.split())).reduce(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the number of words in the line with most words in another way\n",
    "max(map(lambda line: len(line.split()), open(\"/spark/README.md\").read().strip().split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand `flatMap`, I needed to use an [action](https://spark.apache.org/docs/latest/programming-guide.html#actions) to convert the RDD to a list. The solution: `collect`.  We can use `take(n)` instead to grab only the first `n` items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flatmap\n",
    "textFile.flatMap(lambda line: line.split()).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a map/reduce workflow\n",
    "\n",
    "wordCounts = textFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "wordCounts.takeSample(False,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parallelized collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Creating RDDs](http://my.safaribooksonline.com/book/databases/business-intelligence/9781449359034/3dot-programming-with-rdds/_creating_rdds_html):\n",
    "\n",
    "> The simplest way to create RDDs is to take an existing in-memory collection and pass it to SparkContext’s `parallelize` method. This approach is very useful when learning Spark, since you can quickly create your own RDDs in the shell and perform operations on them. Keep in mind however, that outside of prototyping and testing, this is not widely used since it **requires you have your entire dataset in memory on one machine** [emphasis mine]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x: x * x).collect()\n",
    "for num in squared:\n",
    "    print (\"%i \" % (num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code drawn from Karau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. *Learning Spark*. O’Reilly Media, Inc., 2015. http://my.safaribooksonline.com/book/databases/business-intelligence/9781449359034.\n",
    "\n",
    "Key concepts from [Chap 3](http://my.safaribooksonline.com/book/databases/business-intelligence/9781449359034/3dot-programming-with-rdds/_rdd_basics_html):\n",
    "\n",
    "> An RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java or Scala objects, including user-defined classes. Users create RDDs in two ways: by loading an external dataset, or by distributing a collection of objects (e.g. a list or set) in their driver program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:  this won't work because the rdhyee/ipython-spark needs to have hadoop installed, which it doesn't**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3file = sc.textFile(\"s3n://AKIAI3ZHCGO3UMYFXWFA:w0ALUVQ3p6bqmMYytMn1w93fL5JlSLNK5IDKjHRv@aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-52/segment.paths.gz\")\n",
    "s3file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
